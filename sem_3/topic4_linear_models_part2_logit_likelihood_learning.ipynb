{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Юрий Кашницкий\n",
    "    \n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/company/ods/blog/322626/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275d934c160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
